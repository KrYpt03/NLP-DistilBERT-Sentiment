# -*- coding: utf-8 -*-
"""sentiment_analysis_with_distilbert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16IBmC90B8LmpxX3xv5s2r1KrD5OkNh53
"""

# Install Hugging Face Transformers for models and tokenizers
# Install Hugging Face Datasets for easy data loading
# Install accelerate for faster training with Hugging Face Trainer
# Install scikit-learn for evaluation metrics (like accuracy)

!pip install transformers datasets accelerate scikit-learn

from datasets import load_dataset

# Downgrade fsspec to resolve the glob pattern issue
# This is a temporary fix for a known compatibility issue
!pip install fsspec==2023.9.0

# Load the full IMDB dataset
# This might take a minute or two depending on your internet connection
print("Loading dataset...")
full_dataset = load_dataset("imdb")
print("Dataset loaded!")

# --- IMPORTANT FOR SPEED ---
# To make this project finish quickly, we'll use a small subset of the data.
# This is crucial for completing it in less than a day.
# In a real project, you'd use all data, but for learning, this is perfect.

# Select the first 1000 samples for training and 200 for testing
train_dataset = full_dataset["train"].shuffle(seed=42).select(range(1000))
eval_dataset = full_dataset["test"].shuffle(seed=42).select(range(200)) # Using 'test' for evaluation

print(f"Training samples: {len(train_dataset)}")
print(f"Evaluation samples: {len(eval_dataset)}")

# Let's look at one example
print("\nExample from training data:")
print(train_dataset[0])

from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load the tokenizer for DistilBERT
# This converts text into numbers that the model understands
print("Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
print("Tokenizer loaded!")

# Load the DistilBERT model pre-trained for sequence classification
# num_labels=2 because we have two categories: positive (1) and negative (0)
print("Loading model...")
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)
print("Model loaded!")

def tokenize_function(examples):
    # Apply the tokenizer to the 'text' column of the dataset
    # truncation=True: Cut off reviews longer than the model's max input length (usually 512 tokens)
    # padding=True: Pad shorter reviews with zeros so all inputs have the same length (important for batching)
    return tokenizer(examples["text"], truncation=True, padding=True)

print("Tokenizing datasets...")
tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)
tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)
print("Tokenization complete!")

# --- Prepare data for the Hugging Face Trainer ---
# The Trainer expects input columns named 'input_ids', 'attention_mask', and 'labels'.
# Our dataset has 'text' and 'label', so we need to rename/remove columns.
tokenized_train_dataset = tokenized_train_dataset.remove_columns(["text"])
tokenized_eval_dataset = tokenized_eval_dataset.remove_columns(["text"])

# Rename 'label' column to 'labels'
tokenized_train_dataset = tokenized_train_dataset.rename_column("label", "labels")
tokenized_eval_dataset = tokenized_eval_dataset.rename_column("label", "labels")

# Set the format to PyTorch tensors
tokenized_train_dataset.set_format("torch")
tokenized_eval_dataset.set_format("torch")

print("\nExample of tokenized data (first 50 input_ids):")
print(tokenized_train_dataset[0]['input_ids'][:50])
print(tokenized_train_dataset[0]['labels'])

!pip install --upgrade datasets huggingface_hub

import numpy as np
# Import load_metric from the evaluate library instead of datasets
from evaluate import load

# Load the accuracy metric
accuracy_metric = load("accuracy")

# Function to compute metrics for the Trainer
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    # Convert model's output (logits) to predicted class IDs (0 or 1)
    predictions = np.argmax(logits, axis=-1)
    # Compute accuracy using the loaded metric
    return accuracy_metric.compute(predictions=predictions, references=labels)

print("Metrics function defined.")

from transformers import TrainingArguments, Trainer

# Define training arguments
# These control various aspects of the training process
training_args = TrainingArguments(
    output_dir="./results", # Directory to save checkpoints and results
    eval_strategy="epoch", # Evaluate performance after each epoch
    learning_rate=2e-5, # Controls how big of a step the model takes when updating its weights
    per_device_train_batch_size=16, # Number of samples processed in one training step
    per_device_eval_batch_size=16, # Number of samples processed in one evaluation step
    num_train_epochs=3, # Number of times to loop through the entire training dataset (KEEP THIS LOW for speed!)
    weight_decay=0.01, # Regularization to prevent overfitting
    logging_dir='./logs', # Directory for logs
    logging_steps=10, # Log progress every 10 steps
    report_to="none" # Disable reporting to external tools for simplicity
)

# Create the Trainer instance
# This object orchestrates the entire training process
trainer = Trainer(
    model=model, # The model we want to fine-tune
    args=training_args, # The training arguments we defined
    train_dataset=tokenized_train_dataset, # Our prepared training data
    eval_dataset=tokenized_eval_dataset, # Our prepared evaluation data
    tokenizer=tokenizer, # The tokenizer (used for padding if not done already)
    compute_metrics=compute_metrics, # Our function to calculate metrics
)

# Start training! This will take some time (minutes, not hours, with subsetted data)
print("Starting training...")
trainer.train()
print("Training complete!")

# Evaluate the final model on the evaluation set
print("\nEvaluating final model...")
results = trainer.evaluate()
print(results)

from transformers import pipeline

# Create a sentiment analysis pipeline using your fine-tuned model and tokenizer
sentiment_analyzer = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer, device=0) # device=0 means use GPU if available

# Test with some new reviews
reviews = [
    "This movie was absolutely fantastic! I loved every moment of it. Highly recommend.",
    "Boring and predictable. I fell asleep halfway through. A complete waste of time.",
    "It was okay, not great, not terrible. Just average.",
    "The acting was superb, but the plot was a bit confusing."
]

print("\n--- Making predictions on new text ---")
for review in reviews:
    result = sentiment_analyzer(review)[0] # Get the first (and only) result
    print(f"Review: '{review}'")
    print(f"Prediction: {result['label']} (Score: {result['score']:.4f})\n")

# Save the model and tokenizer to a directory
output_model_dir = "./my_fine_tuned_sentiment_model"
trainer.save_model(output_model_dir)
tokenizer.save_pretrained(output_model_dir)

print(f"Model and tokenizer saved to: {output_model_dir}")